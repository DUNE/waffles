{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import waffles.input.raw_hdf5_reader as reader\n",
    "from waffles.np04_analysis.LED_calibration.run_number_to_LED_configuration import run_to_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some convenient definitions\n",
    "\n",
    "def dump_object_to_pickle(\n",
    "        object, \n",
    "        saving_folderpath : str,\n",
    "        output_filename : str,\n",
    "        verbose : bool = True) -> None:\n",
    "    \"\"\"This function gets the following positional argument:\n",
    "\n",
    "    - object\n",
    "    - saving_folderpath (str): Path to the folder\n",
    "    where to save the file.\n",
    "    - output_filename (str): Name of the output \n",
    "    pickle file.\n",
    "\n",
    "    And the following keyword argument:\n",
    "\n",
    "    - verbose (bool): Whether to print functioning\n",
    "    related messages.\n",
    "    \n",
    "    It saves the given object, object, to a pickle file \n",
    "    which is stored in the path given by saving_filepath\"\"\"\n",
    "\n",
    "    # If the saving folder does not exist, create it\n",
    "    if not os.path.exists(saving_folderpath):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"In function dump_object_to_pickle(): Folder {saving_folderpath} does not exist. It will be created.\")\n",
    "\n",
    "        os.makedirs(saving_folderpath)\n",
    "\n",
    "    # Create the output filepath\n",
    "    output_filepath = os.path.join(\n",
    "        saving_folderpath, \n",
    "        output_filename)\n",
    "    \n",
    "    with open(\n",
    "        output_filepath, \n",
    "        \"wb\") as output_file:\n",
    "\n",
    "        pickle.dump(object, output_file)\n",
    "\n",
    "        return\n",
    "\n",
    "def save_run_to_pickled_WaveformSet(\n",
    "    run : int,\n",
    "    saving_folderpath : str,\n",
    "    average_wfs_per_channel : int = 4000,\n",
    "    channels_no : int = 40,\n",
    "    rucio_filepaths_folderpath : str = \"/eos/experiment/neutplatform/protodune/experiments/ProtoDUNE-II/PDS_Commissioning/waffles/1_rucio_paths/\",\n",
    "    read_full_streaming_data : bool = False,\n",
    "    subsample_seed : int = 3,\n",
    "    verbose : bool = True):\n",
    "    \"\"\"This function gets the following positional arguments:\n",
    "\n",
    "    - run (int): Number of the run whose data we want to convert\n",
    "    to a pickle'd WaveformSet.\n",
    "    - saving_folderpath (str): Path to the folder where to save\n",
    "    the pickle'd WaveformSet(s).\n",
    "\n",
    "    This function gets the following keyword arguments:\n",
    "\n",
    "    - average_wfs_per_channel (int): Assuming that the read data\n",
    "    is homogeneously distributed along the detector channels,\n",
    "    the pickle'd WaveformSet object(s) will contain, on average,\n",
    "    average_wfs_per_channel Waveform objects per detector\n",
    "    channel.\n",
    "    - channels_no (int): Number of channels in the detector.\n",
    "    - rucio_filepaths_folderpath (str): Path to the folder\n",
    "    where the files with the rucio filepaths are stored.\n",
    "    The file which contains the rucio filepaths for a\n",
    "    given run number, <run>, is assumed to be called \n",
    "    '0<run>.txt',\n",
    "    - read_full_streaming_data (bool): Whether to read the\n",
    "    full-streaming data of the self-trigger data.\n",
    "    - subsample_seed (int): The seed for the subsample\n",
    "    parameter. This parameter is decreased unit by unit\n",
    "    until the number of Waveform objects in the resulting\n",
    "    WaveformSet object reaches\n",
    "    average_wfs_per_channel * channels_no. This parameter\n",
    "    is given to the 'subsample' parameter of the\n",
    "    WaveformSet_from_hdf5_file() function. Check such function\n",
    "    docstring for more information.\n",
    "    - verbose (bool): Whether to print functioning-related\n",
    "    messages.\n",
    "    \n",
    "    This function looks for a file called '0<run>.txt' within\n",
    "    the folder whose path is given by \n",
    "    rucio_filepaths_folderpath. Such file is assumed to be a\n",
    "    text file with a list of filepaths. Parsing such file is\n",
    "    delegated to the get_filepaths_from_rucio() function of\n",
    "    'raw_hdf5_reader.py' module. If it is found, then \n",
    "    it starts reading WaveformSet(s), one per filepath, until\n",
    "    the total number of read waveforms have reached\n",
    "    average_wfs_per_channel * channels_no waveforms. Reading\n",
    "    the WaveformSet(s) is delegated to the\n",
    "    WaveformSet_from_hdf5_file() function of the \n",
    "    'raw_hdf5_reader.py' module. The read WaveformSet(s) are \n",
    "    pickle'd to files which are saved in the folder pointed \n",
    "    to by saving_folderpath. The WaveformSet coming from \n",
    "    the i-th filepath is saved to the file named\n",
    "    'run_<run>_chunk_<i>.pkl'\n",
    "    \"\"\"\n",
    "\n",
    "    aux = rucio_filepaths_folderpath+f\"/0{run}.txt\"\n",
    "\n",
    "    try:\n",
    "        rucio_filepaths = reader.get_filepaths_from_rucio(aux)\n",
    "    # Happens if there are no rucio filepaths for this run in rucio_filepaths_folderpath\n",
    "    except Exception:\n",
    "        print(\n",
    "            f\"--> WARNING: Did not find the rucio paths for run {run}. Ending execution \"\n",
    "            f\"of save_run_to_pickled_WaveformSet({run}, ...).\")\n",
    "        return\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"--> Processing run {run}: Found {len(rucio_filepaths)} chunks...\")\n",
    "\n",
    "    fGoForAnotherChunk = True\n",
    "    wvfs_left_to_read_for_this_run = average_wfs_per_channel * channels_no\n",
    "    current_chunk_iterator = 0\n",
    "\n",
    "    while fGoForAnotherChunk:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\t --> Processing chunk {current_chunk_iterator+1}/{len(rucio_filepaths)} ...\")\n",
    "\n",
    "        subsample = subsample_seed\n",
    "        fReadSameChunkAgain = True\n",
    "\n",
    "        while fReadSameChunkAgain:\n",
    "\n",
    "            aux_wfset = reader.WaveformSet_from_hdf5_file(\n",
    "                rucio_filepaths[current_chunk_iterator],\n",
    "                read_full_streaming_data=read_full_streaming_data,\n",
    "                subsample=subsample,\n",
    "                # WaveformSet_from_hdf5_file apparently subsamples from\n",
    "                # the [0, wvfm_count] range. Therefore, if we set\n",
    "                # wvfm_count to wvfs_left_to_read_for_this_run we\n",
    "                # will get, at most, wvfs_left_to_read_for_this_run/subsample\n",
    "                wvfm_count=wvfs_left_to_read_for_this_run*subsample,\n",
    "                )\n",
    "            \n",
    "            # In this case, we already have what we need for this run\n",
    "            if len(aux_wfset.waveforms) == wvfs_left_to_read_for_this_run:\n",
    "                fReadSameChunkAgain = False\n",
    "                fGoForAnotherChunk = False\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"--> Got enough waveforms ({len(aux_wfset.waveforms)}) \"\n",
    "                          f\"from chunk {current_chunk_iterator+1}/{len(rucio_filepaths)} \"\n",
    "                          f\"of run {run}\")\n",
    "                    print(f\"--> Now saving it to a pickle file ...\")\n",
    "\n",
    "                dump_object_to_pickle(\n",
    "                    aux_wfset,\n",
    "                    saving_folderpath,\n",
    "                    f\"run_{run}_chunk_{current_chunk_iterator}.pkl\",\n",
    "                    verbose=verbose)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"--> Successfully saved WaveformSet of run {run}\")\n",
    "\n",
    "            # In this case, we need more waveforms for this run\n",
    "            elif len(aux_wfset.waveforms) < wvfs_left_to_read_for_this_run:\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"--> Didn't get enough waveforms from chunk \"\n",
    "                          f\"{current_chunk_iterator+1}/{len(rucio_filepaths)} \"\n",
    "                          f\"of run {run}\")\n",
    "                    print(f\"--> Expected {wvfs_left_to_read_for_this_run}, but only read {len(aux_wfset.waveforms)}\")\n",
    "\n",
    "                # In this case, try to read the same file but with a finer subsampling\n",
    "                if subsample > 1:\n",
    "                    # fReadSameChunkAgain is True by default\n",
    "                    subsample -= 1\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"--> Switching 'subsample' from {subsample+1} to {subsample} and reading it again...\")\n",
    "\n",
    "                # In this case, we read every waveform from this chunk, but we still\n",
    "                # haven't got enough waveforms, so go for the following chunk\n",
    "                else:\n",
    "                    subsample = subsample_seed\n",
    "                    fReadSameChunkAgain = False\n",
    "                    # fGoForAnotherChunk is True by default\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"--> All of the waveforms from this chunk were read\")\n",
    "                        print(f\"--> Saving them and proceeding to look for \"\n",
    "                              f\"{wvfs_left_to_read_for_this_run-len(aux_wfset.waveforms)} \"\n",
    "                              f\"(={wvfs_left_to_read_for_this_run}-{len(aux_wfset.waveforms)}) \"\n",
    "                              f\"waveforms from the following chunk ({current_chunk_iterator+2}/{len(rucio_filepaths)}) \"\n",
    "                              f\"of this run ({run}).\")\n",
    "\n",
    "                    dump_object_to_pickle(\n",
    "                        aux_wfset,\n",
    "                        saving_folderpath,\n",
    "                        f\"run_{run}_chunk_{current_chunk_iterator}.pkl\",\n",
    "                        verbose=verbose)\n",
    "\n",
    "                    # Switch to next chunk\n",
    "                    current_chunk_iterator += 1\n",
    "                    # But only read the waveforms that we need to add up to \n",
    "                    # average_wfs_per_channel * channels_no\n",
    "                    wvfs_left_to_read_for_this_run -= len(aux_wfset.waveforms)\n",
    "                    \n",
    "            # In this case, WaveformSet_from_hdf5_file() is misbehaving\n",
    "            else:\n",
    "                raise Exception(f\"WaveformSet_from_hdf5_file() is misbehaving. It read\"\n",
    "                                f\" more waveforms ({len(aux_wfset.waveforms)}) than \"\n",
    "                                f\"specified (wvfm_count={average_wfs_per_channel * channels_no})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_no = 2\n",
    "apa_no = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_folderpath = \"/afs/cern.ch/work/j/jurenago/private/repositories/waffles/src/waffles/np04_analysis/LED_calibration/pickles\"\n",
    "average_wfs_per_channel = 4000\n",
    "channels_per_apa = 40\n",
    "acquired_apas = 2\n",
    "rucio_filepaths_folderpath = \"/eos/experiment/neutplatform/protodune/experiments/ProtoDUNE-II/PDS_Commissioning/waffles/1_rucio_paths/\"\n",
    "subsample_seed = 3\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t --> Now retrieving data for run = 28176\n",
      "\n",
      "Your files are stored around the world. \n",
      "[WARNING] Check you have a correct configuration to use XRootD\n",
      "--> Processing run 28176: Found 8 chunks...\n",
      "\t --> Processing chunk 1/8 ...\n",
      "Using XROOTD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run: [ERROR] Local error: file exists:  (destination)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_numb= 28176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [00:27,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Got enough waveforms (160000) from chunk 1/8 of run 28176\n",
      "--> Now saving it to a pickle file ...\n",
      "--> Successfully saved WaveformSet of run 28176\n",
      "\t --> Now retrieving data for run = 28177\n",
      "--> WARNING: Did not find the rucio paths for run 28177. Ending execution of save_run_to_pickled_WaveformSet(28177, ...).\n",
      "\t --> Now retrieving data for run = 28179\n",
      "\n",
      "Your files are stored around the world. \n",
      "[WARNING] Check you have a correct configuration to use XRootD\n",
      "--> Processing run 28179: Found 8 chunks...\n",
      "\t --> Processing chunk 1/8 ...\n",
      "Using XROOTD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run: [ERROR] Local error: file exists:  (destination)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_numb= 28179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [00:28,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Got enough waveforms (160000) from chunk 1/8 of run 28179\n",
      "--> Now saving it to a pickle file ...\n",
      "--> Successfully saved WaveformSet of run 28179\n",
      "\t --> Now retrieving data for run = 28180\n",
      "\n",
      "Your files are stored around the world. \n",
      "[WARNING] Check you have a correct configuration to use XRootD\n",
      "--> Processing run 28180: Found 8 chunks...\n",
      "\t --> Processing chunk 1/8 ...\n",
      "Using XROOTD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run: [ERROR] Local error: file exists:  (destination)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_numb= 28180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:06,  5.20it/s]HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n",
      "HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n",
      "HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n",
      "HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n",
      "HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n",
      "37it [00:06,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c32267df0>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c30d4f170>\n",
      "(280, 0)\n",
      "47244771330\n",
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c32267df0>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c30d4f170>\n",
      "(280, 0)\n",
      "51539738626\n",
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c28180af0>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c30d4f0f0>\n",
      "(288, 0)\n",
      "47244771330\n",
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c28180af0>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c30d4f0f0>\n",
      "(288, 0)\n",
      "51539738626\n",
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c31098270>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c32746fb0>\n",
      "(296, 0)\n",
      "47244771330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [00:25,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Got enough waveforms (160000) from chunk 1/8 of run 28180\n",
      "--> Now saving it to a pickle file ...\n",
      "--> Successfully saved WaveformSet of run 28180\n",
      "\t --> Now retrieving data for run = 28181\n",
      "\n",
      "Your files are stored around the world. \n",
      "[WARNING] Check you have a correct configuration to use XRootD\n",
      "--> Processing run 28181: Found 8 chunks...\n",
      "\t --> Processing chunk 1/8 ...\n",
      "Using XROOTD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run: [ERROR] Local error: file exists:  (destination)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_numb= 28181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:06,  6.78it/s]HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n",
      "HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n",
      "HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n",
      "HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n",
      "HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n",
      "37it [00:07, 11.90it/s]HDF5-DIAG: Error detected in HDF5 (1.12.0) thread 0:\n",
      "  #000: /tmp/root/spack-stage/spack-stage-hdf5-1.12.0-nwlxllzr7neqtcsd575mrf2jhie4sfbd/spack-src/src/H5D.c line 280 in H5Dopen2(): name parameter cannot be an empty string\n",
      "    major: Invalid arguments to routine\n",
      "    minor: Bad value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c54525330>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c2b9b8030>\n",
      "(280, 0)\n",
      "55834705922\n",
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c54525330>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c2b9b8030>\n",
      "(280, 0)\n",
      "47244771330\n",
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c54525330>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c2b9b8030>\n",
      "(280, 0)\n",
      "51539738626\n",
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c322fb730>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c322fb0b0>\n",
      "(288, 0)\n",
      "47244771330\n",
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c322fb730>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c322fb0b0>\n",
      "(288, 0)\n",
      "51539738626\n",
      "Corrupted fragment:\n",
      " <daqdataformats._daq_daqdataformats_py.Fragment object at 0x7f8c2ba5f570>\n",
      "<daqdataformats._daq_daqdataformats_py.TriggerRecordHeader object at 0x7f8c33113830>\n",
      "(296, 0)\n",
      "51539738626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [00:25,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Got enough waveforms (160000) from chunk 1/8 of run 28181\n",
      "--> Now saving it to a pickle file ...\n",
      "--> Successfully saved WaveformSet of run 28181\n"
     ]
    }
   ],
   "source": [
    "pde = 0.5\n",
    "for run in run_to_config[batch_no][3][pde].keys():\n",
    "\n",
    "    print(f\"\\t --> Now retrieving data for run = {run}\")\n",
    "    \n",
    "    aux = saving_folderpath + f\"/batch_{batch_no}/apa_{apa_no}/{pde}/\"\n",
    "\n",
    "    save_run_to_pickled_WaveformSet(\n",
    "        run,\n",
    "        aux,\n",
    "        average_wfs_per_channel=average_wfs_per_channel,\n",
    "        channels_no=channels_per_apa*acquired_apas,\n",
    "        rucio_filepaths_folderpath=rucio_filepaths_folderpath,\n",
    "        read_full_streaming_data=True if apa_no == 1 else False,\n",
    "        subsample_seed=subsample_seed,\n",
    "        verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Now retrieving data for PDE = 0.4\n",
      "\t --> Now retrieving data for run = 28148\n",
      "\n",
      "Your files are stored around the world. \n",
      "[WARNING] Check you have a correct configuration to use XRootD\n",
      "--> Processing run 28148: Found 8 chunks...\n",
      "\t --> Processing chunk 1/8 ...\n",
      "Using XROOTD\n",
      "run_numb= 28148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "357it [00:04, 85.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Got enough waveforms (160000) from chunk 1/8 of run 28148\n",
      "--> Now saving it to a pickle file ...\n",
      "In function dump_object_to_pickle(): Folder /afs/cern.ch/work/j/jurenago/private/repositories/waffles/src/waffles/np04_analysis/LED_calibration/pickles/batch_2/apa_2/0.4/ does not exist. It will be created.\n",
      "--> Successfully saved WaveformSet of run 28148\n",
      "\t --> Now retrieving data for run = 28149\n",
      "--> WARNING: Did not find the rucio paths for run 28149. Ending execution of save_run_to_pickled_WaveformSet(28149, ...).\n",
      "\t --> Now retrieving data for run = 28150\n",
      "\n",
      "Your files are stored around the world. \n",
      "[WARNING] Check you have a correct configuration to use XRootD\n",
      "--> Processing run 28150: Found 8 chunks...\n",
      "\t --> Processing chunk 1/8 ...\n",
      "Using XROOTD\n",
      "run_numb= 28150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "356it [00:04, 84.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Got enough waveforms (160000) from chunk 1/8 of run 28150\n",
      "--> Now saving it to a pickle file ...\n",
      "--> Successfully saved WaveformSet of run 28150\n",
      "\t --> Now retrieving data for run = 28151\n",
      "\n",
      "Your files are stored around the world. \n",
      "[WARNING] Check you have a correct configuration to use XRootD\n",
      "--> Processing run 28151: Found 8 chunks...\n",
      "\t --> Processing chunk 1/8 ...\n",
      "Using XROOTD\n",
      "run_numb= 28151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "357it [00:04, 86.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Got enough waveforms (160000) from chunk 1/8 of run 28151\n",
      "--> Now saving it to a pickle file ...\n",
      "--> Successfully saved WaveformSet of run 28151\n",
      "\t --> Now retrieving data for run = 28152\n",
      "\n",
      "Your files are stored around the world. \n",
      "[WARNING] Check you have a correct configuration to use XRootD\n",
      "--> Processing run 28152: Found 8 chunks...\n",
      "\t --> Processing chunk 1/8 ...\n",
      "Using XROOTD\n",
      "run_numb= 28152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "356it [00:04, 85.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Got enough waveforms (160000) from chunk 1/8 of run 28152\n",
      "--> Now saving it to a pickle file ...\n",
      "--> Successfully saved WaveformSet of run 28152\n",
      "\t --> Now retrieving data for run = 28153\n",
      "\n",
      "Your files are stored around the world. \n",
      "[WARNING] Check you have a correct configuration to use XRootD\n",
      "--> Processing run 28153: Found 8 chunks...\n",
      "\t --> Processing chunk 1/8 ...\n",
      "Using XROOTD\n",
      "run_numb= 28153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "356it [00:04, 82.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Got enough waveforms (160000) from chunk 1/8 of run 28153\n",
      "--> Now saving it to a pickle file ...\n",
      "--> Successfully saved WaveformSet of run 28153\n",
      "--> Now retrieving data for PDE = 0.45\n",
      "\t --> Now retrieving data for run = 28159\n",
      "\n",
      "Your files are stored around the world. \n",
      "[WARNING] Check you have a correct configuration to use XRootD\n",
      "--> Processing run 28159: Found 8 chunks...\n",
      "\t --> Processing chunk 1/8 ...\n",
      "Using XROOTD\n"
     ]
    }
   ],
   "source": [
    "for pde in run_to_config[batch_no][apa_no].keys():\n",
    "\n",
    "    print(f\"--> Now retrieving data for PDE = {pde}\")\n",
    "\n",
    "    for run in run_to_config[batch_no][apa_no][pde].keys():\n",
    "\n",
    "        print(f\"\\t --> Now retrieving data for run = {run}\")\n",
    "        \n",
    "        aux = saving_folderpath + f\"/batch_{batch_no}/apa_{apa_no}/{pde}/\"\n",
    "\n",
    "        save_run_to_pickled_WaveformSet(\n",
    "            run,\n",
    "            aux,\n",
    "            average_wfs_per_channel=average_wfs_per_channel,\n",
    "            channels_no=channels_no,\n",
    "            rucio_filepaths_folderpath=rucio_filepaths_folderpath,\n",
    "            read_full_streaming_data=True if apa_no == 1 else False,\n",
    "            subsample_seed=subsample_seed,\n",
    "            verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(runs)):\n",
    "\n",
    "    aux = rucio_filepaths_folderpath+f\"/0{runs[i]}.txt\"\n",
    "\n",
    "    try:\n",
    "        rucio_filepaths = reader.get_filepaths_from_rucio(aux)\n",
    "    # Happens if there are no rucio filepaths for this run in rucio_filepaths_folderpath\n",
    "    except Exception:\n",
    "        print(f\"--> WARNING: Did not find the rucio paths for run {runs[i]}. Skipping this run.\")\n",
    "        continue\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"--> Processing run {runs[i]} ({i+1}/{len(runs)}): Found {len(rucio_filepaths)} chunks...\")\n",
    "\n",
    "    fGoForAnotherChunk = True\n",
    "    wvfs_left_to_read_for_this_run = average_wfs_per_channel * channels_no\n",
    "    current_chunk_iterator = 0\n",
    "\n",
    "    while fGoForAnotherChunk:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\t --> Processing chunk {current_chunk_iterator+1}/{len(rucio_filepaths)} ...\")\n",
    "\n",
    "        subsample = subsample_seed\n",
    "        fReadSameChunkAgain = True\n",
    "\n",
    "        while fReadSameChunkAgain:\n",
    "\n",
    "            aux_wfset = reader.WaveformSet_from_hdf5_file( \n",
    "                rucio_filepaths[current_chunk_iterator],\n",
    "                read_full_streaming_data=read_full_streaming_data\n",
    "                subsample=subsample,\n",
    "                # WaveformSet_from_hdf5_file apparently subsamples from\n",
    "                # the [0, wvfm_count] range. Therefore, if we set\n",
    "                # wvfm_count to wvfs_left_to_read_for_this_run we\n",
    "                # will get, at most, wvfs_left_to_read_for_this_run/subsample\n",
    "                wvfm_count=wvfs_left_to_read_for_this_run*subsample,\n",
    "                )\n",
    "            \n",
    "            # In this case, we already have what we need for this run\n",
    "            if len(aux_wfset.waveforms) == wvfs_left_to_read_for_this_run:\n",
    "                fReadSameChunkAgain = False\n",
    "                fGoForAnotherChunk = False\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"--> Got enough waveforms ({len(aux_wfset.waveforms)}) \"\n",
    "                          f\"from chunk {current_chunk_iterator+1}/{len(rucio_filepaths)} \"\n",
    "                          f\"of run {runs[i]}\")\n",
    "                    print(f\"--> Now saving it to a pickle file ...\")\n",
    "\n",
    "                dump_object_to_pickle(\n",
    "                    aux_wfset,\n",
    "                    saving_folderpath+f\"{runs[i]}_chunk_{current_chunk_iterator}.pkl\")\n",
    "                \n",
    "                if verbose:\n",
    "                    try:\n",
    "                        print(f\"--> Switching to next run {runs[i+1]}\")\n",
    "                    # Happens if all runs were already processed\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "\n",
    "            # In this case, we need more waveforms for this run\n",
    "            elif len(aux_wfset.waveforms) < wvfs_left_to_read_for_this_run:\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"--> Didn't get enough waveforms from chunk \"\n",
    "                          f\"{current_chunk_iterator+1}/{len(rucio_filepaths)} \"\n",
    "                          f\"of run {runs[i]}\")\n",
    "                    print(f\"--> Expected {wvfs_left_to_read_for_this_run}, but only read {len(aux_wfset.waveforms)}\")\n",
    "\n",
    "                # In this case, try to read the same file but with a finer subsampling\n",
    "                if subsample > 1:\n",
    "                    # fReadSameChunkAgain is True by default\n",
    "                    subsample -= 1\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"--> Switching 'subsample' from {subsample+1} to {subsample} and reading it again...\")\n",
    "\n",
    "                # In this case, we read every waveform from this chunk, but we still\n",
    "                # haven't got enough waveforms, so go for the following chunk\n",
    "                else:\n",
    "                    subsample = subsample_seed\n",
    "                    fReadSameChunkAgain = False\n",
    "                    # fGoForAnotherChunk is True by default\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"--> All of the waveforms from this chunk were read\")\n",
    "                        print(f\"--> Saving them and proceeding to look for \"\n",
    "                              f\"{wvfs_left_to_read_for_this_run-len(aux_wfset.waveforms)} \"\n",
    "                              f\"(={wvfs_left_to_read_for_this_run}-{len(aux_wfset.waveforms)}) \"\n",
    "                              f\"waveforms from the following chunk ({current_chunk_iterator+2}/{len(rucio_filepaths)}) \"\n",
    "                              f\"of this run ({runs[i]}).\")\n",
    "\n",
    "                    dump_object_to_pickle(\n",
    "                        aux_wfset,\n",
    "                        saving_folderpath+f\"{runs[i]}_chunk_{current_chunk_iterator}.pkl\")\n",
    "\n",
    "                    # Switch to next chunk\n",
    "                    current_chunk_iterator += 1\n",
    "                    # But only read the waveforms that we need to add up to \n",
    "                    # average_wfs_per_channel * channels_no\n",
    "                    wvfs_left_to_read_for_this_run -= len(aux_wfset.waveforms)\n",
    "                    \n",
    "            # In this case, WaveformSet_from_hdf5_file() is misbehaving\n",
    "            else:\n",
    "                raise Exception(f\"WaveformSet_from_hdf5_file() read more waveforms\"\n",
    "                                f\" ({len(aux_wfset.waveforms)}) than specified (wvfm_count=\"\n",
    "                                f\"{average_wfs_per_channel * channels_no})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
