{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALIZZARE UN MASSIMO DI 10 FILE\n",
    "# Utilizzare poi merging_results_apa12.ipynb per unire i risultati\n",
    "\n",
    "#DONE:\n",
    "# Energy +5 GeV - 27367 : first 71 files analized\n",
    "# Energy +2 GeV - 27355 : first 70 files analized\n",
    "# Energy +1 GeV - 27343 : first 70 files analized\n",
    "# Energy +3 GeV - 27361: first 70 files analized\n",
    "# # Energy +5 GeV - 27374: first 70 files analized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import waffles.np04_analysis.lightyield_vs_energy.scripts.utils as utils_module\n",
    "from waffles.np04_analysis.lightyield_vs_energy.scripts.utils import *\n",
    "\n",
    "importlib.reload(utils_module)\n",
    "from waffles.np04_analysis.lightyield_vs_energy.scripts.utils import *\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_directory = \"/afs/cern.ch/work/a/anbalbon/private/waffles/src/waffles/np04_analysis/lightyield_vs_energy/scripts/Renan_scripts/v_27xxx_fix_avg\"\n",
    "output_dir = \"/afs/cern.ch/work/a/anbalbon/private/waffles/src/waffles/np04_analysis/lightyield_vs_energy/output/apa1_vs_apa2\"\n",
    "rucio_hdf5_dir = \"/afs/cern.ch/work/a/anbalbon/public/reading_rucio\"\n",
    "# rucio_hdf5_dir = \"/eos/user/a/anbalbon/reading_rucio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = 7\n",
    "run_number = 27374\n",
    "reading_type = 'single' #[\"single\", \"merged\"]\n",
    "\n",
    "if reading_type == 'single': #reading single hdf5 files and merging it, without saving the merged file\n",
    "    event_type = 'no_selection' # [\"beam_only\", \"no_selection\"] \n",
    "    n_files_start = 65\n",
    "    n_files_stop = 70\n",
    "\n",
    "elif reading_type == 'merged': #reading merged pkl file, produced by hdf5_run_merging_NEW.py\n",
    "    FS_merged_filepath = ''\n",
    "    ST_merged_filepath = ''\n",
    "    \n",
    "    # 3 GeV \n",
    "    # FS: /afs/cern.ch/work/a/anbalbon/public/reading_rucio/run027361/run027361_full_streaming_no_selection_1st.pkl\n",
    "    # ST: /afs/cern.ch/work/a/anbalbon/public/reading_rucio/run027361/run027361_self_trigger_no_selection_1st.pkl\n",
    "    \n",
    "    # 5 GeV \n",
    "    # ST: /afs/cern.ch/work/a/anbalbon/public/reading_rucio/run027367run027367_self_trigger_no_selection_51files.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura file SELF-TRIGGER\n",
    "\n",
    "if reading_type == 'merged': \n",
    "    print(\"Loading Self-Trigger Wfset...\", end='')\n",
    "    with open(ST_merged_filepath, 'rb') as f:\n",
    "        ST_wfset = pickle.load(f) \n",
    "        files_read = []\n",
    "        print('\\t --> \\t ST done\\n')  \n",
    "else:\n",
    "    ST_wfset, files_read = reading_merge_hdf5(run_number, 'self_trigger', event_type, rucio_hdf5_dir,  n_files_start, n_files_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura file FULL-STREAMING\n",
    "\n",
    "if reading_type == 'merged': \n",
    "    print(\"Loading Full-Streaming Wfset...\", end='')\n",
    "    with open(FS_merged_filepath, 'rb') as f:\n",
    "        FS_wfset = pickle.load(f) \n",
    "        files_read = []\n",
    "        print('\\t --> \\t FS done\\n')\n",
    "else:\n",
    "    FS_wfset, files_read = reading_merge_hdf5(run_number, 'full_streaming', event_type, rucio_hdf5_dir, n_files_start, n_files_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing wfset\n",
    "time_info_dic = {}\n",
    "wfset_dic = {}\n",
    "for trigger_mode, apa, wfset in zip(['full_streaming', 'self_trigger'], [1, 2], [FS_wfset, ST_wfset]):\n",
    "# for trigger_mode, apa, wfset in zip(['full_streaming'], [1], [FS_wfset]):\n",
    "# for trigger_mode, apa, wfset in zip(['self_trigger'], [2], [ST_wfset]):\n",
    "\n",
    "    # 1\n",
    "    # Remove baseline and invert waveforms to have positive peaks (subtract_baseline_invert function) \n",
    "    baseline_analysis_label = 'baseliner'\n",
    "    baseline_input_parameters = IPDict({'baseline_limits': [0, 50], \n",
    "                                            'std_cut': 3., \n",
    "                                            'type': 'mean'})    \n",
    "    baseline_checks_kwargs = IPDict({'points_no': wfset.points_per_wf})\n",
    "    _ = wfset.analyse(baseline_analysis_label, WindowBaseliner, baseline_input_parameters, checks_kwargs=baseline_checks_kwargs, overwrite=True)\n",
    "    wfset.apply(subtract_baseline_invert, baseline_analysis_label, inversion = True, show_progress=False)\n",
    "\n",
    "\n",
    "    # 2\n",
    "    # Search for peaks (MyAnaPeak_NEW wfana)\n",
    "    peak_analysis_class = MyAnaPeak_NEW\n",
    "    peak_analysis_label = 'finding_peaks'\n",
    "    peak_input_parameters = IPDict(baseline_limits= [0, 50],\n",
    "                                    n_std = 5,\n",
    "                                    peak_distance= 500,\n",
    "                                    beam_timeoffset_limits = [-40, -20],\n",
    "                                    signal_sign = \"positive\"\n",
    "                                    )\n",
    "    peak_checks_kwargs = IPDict({'points_no': wfset.points_per_wf})\n",
    "    _ = wfset.analyse(label=peak_analysis_label ,analysis_class=peak_analysis_class, input_parameters=peak_input_parameters, checks_kwargs = peak_checks_kwargs, overwrite=True)\n",
    "\n",
    "\n",
    "    # 3\n",
    "    # Select beam waveforms: a waveform peak must be within a timeoffset range (), only one peak must satify this condition per waveform! (beam_filter filter function)\n",
    "    wfset = WaveformSet.from_filtered_WaveformSet(wfset, beam_filter, peak_analysis_label)\n",
    "    \n",
    "\n",
    "    # 4\n",
    "    # Creating list of offset time and absolute peal time \n",
    "    offset_list = []\n",
    "    beam_peak_absolute_time_list = []\n",
    "    for wf in wfset.waveforms:\n",
    "        offset_list.extend(wf.analyses[peak_analysis_label].result['time_offset'])\n",
    "        beam_peak_absolute_time_list.extend(wf.analyses[peak_analysis_label].result['beam_peak_absolute_time'])\n",
    "    time_info_dic[trigger_mode] = {'offset_list': offset_list, 'beam_peak_absolute_time_list': beam_peak_absolute_time_list}\n",
    "\n",
    "    #save wfset\n",
    "    wfset_dic[apa] = wfset\n",
    "\n",
    "print(f\"FS wfset length --> BEFORE: {len(FS_wfset.waveforms)} \\t AFTER: {len(wfset_dic[1].waveforms)}\")\n",
    "print(f\"ST wfset length --> BEFORE: {len(ST_wfset.waveforms)} \\t AFTER: {len(wfset_dic[2].waveforms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# + cut FS waveform around the peak (cut_full_streaming_window fuction)\n",
    "wfset_dic[1].apply(cut_full_streaming_window, analysis_label = peak_analysis_label ,show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving wfset and list of read files\n",
    "output_analysis_folder = f\"{output_dir}/{energy}GeV/{n_files_start}_to_{n_files_stop}\"\n",
    "os.makedirs(output_analysis_folder, exist_ok=True)\n",
    "\n",
    "print(f\"Saving full-streaming wfset... \", end='')\n",
    "with open(f\"{output_analysis_folder}/FS_wfset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(wfset_dic[1], f)\n",
    "    print('done! \\n')\n",
    "\n",
    "print(f\"Saving self-trigger wfset... \", end='')\n",
    "with open(f\"{output_analysis_folder}/ST_wfset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(wfset_dic[2], f)\n",
    "    print('done! \\n')\n",
    "\n",
    "print(f\"Saving read_files info...\", end='')\n",
    "with open(f\"{output_analysis_folder}/files_read.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in files_read:\n",
    "        f.write(f\"{item}\\n\")\n",
    "    print('done! \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# Search for Full-streaming Self-trigger matching indentifing trigger times (trigger_time_list function)\n",
    "all_beam_peak_absolute_time = sorted(time_info_dic['full_streaming']['beam_peak_absolute_time_list'] + time_info_dic['self_trigger']['beam_peak_absolute_time_list'])\n",
    "print('Number of beam peaks before removing close values: ', len(all_beam_peak_absolute_time))\n",
    "print('Number of beam peaks before removing close values (FS): ', len(time_info_dic['full_streaming']['beam_peak_absolute_time_list']))\n",
    "print('Number of beam peaks before removing close values (ST): ', len(time_info_dic['self_trigger']['beam_peak_absolute_time_list']))\n",
    "print()\n",
    "\n",
    "delta_ts = 100 #minimum timeticks between two triggers \n",
    "trigger_time_list = trigger_time_searching(all_beam_peak_absolute_time, delta=delta_ts)\n",
    "print(\"Number of beam peaks after removing close values (total): \", len(trigger_time_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "# Do a for cycle on each trigger time to extrapolate the mean number of PE for APA 1 and APA 2 \n",
    "\n",
    "photoelectron_dict = {1 : {'channel_dic': [], 'list':[], 'mean':[], 'std':[], 'n_events':[]}, 2 : {'channel_dic': [], 'list':[], 'mean':[], 'std':[], 'n_events':[]}, 'trigger time' : []}\n",
    "\n",
    "for i_trigger in tqdm(range(len(trigger_time_list)), desc=\"Processing triggers\"):\n",
    "    trigger_time = trigger_time_list[i_trigger]\n",
    "\n",
    "    # 6.1\n",
    "    # Select waveforms associated to this trigger timestamp (timestamp_filter filter function)\n",
    "    # both wfset must exist!!!\n",
    "\n",
    "    try:\n",
    "        FS_trigger_wfset = WaveformSet.from_filtered_WaveformSet(wfset_dic[1], timestamp_filter, peak_analysis_label, trigger_time, delta_ts)\n",
    "    except Exception as e:\n",
    "        print(f'Error in FULL-STREAMING trigger wfset filter: {e}')\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        ST_trigger_wfset = WaveformSet.from_filtered_WaveformSet(wfset_dic[2], timestamp_filter, peak_analysis_label, trigger_time, delta_ts)\n",
    "    except Exception as e:\n",
    "        print(f'{i_trigger} trigger --> Error in SELF-TRIGGER trigger wfset filter: {e}')\n",
    "        continue\n",
    "\n",
    "\n",
    "    for trigger_mode, apa, wfset in zip(['full_streaming', 'self_trigger'], [1, 2], [FS_trigger_wfset, ST_trigger_wfset]):\n",
    "    # for trigger_mode, apa, wfset in zip(['full_streaming'], [1, 2], [FS_trigger_wfset]):\n",
    "    # for trigger_mode, apa, wfset in zip(['self_trigger'], [1, 2], [ST_trigger_wfset]):\n",
    "\n",
    "        # 6.2\n",
    "        # Apply convolution to evaluate the number of photoelectrons of each waveform (MyAnaConvolution wfana)\n",
    "        convolution_analysis_class = MyAnaConvolution\n",
    "        convolution_analysis_label = 'searching_pe'\n",
    "        convolution_input_parameters = IPDict(df_template = reading_template(template_directory),\n",
    "                                                tick = 265) #???\n",
    "        convolution_checks_kwargs = IPDict({'points_no': wfset.points_per_wf})\n",
    "        _ = wfset.analyse( label=convolution_analysis_label, analysis_class=convolution_analysis_class, input_parameters=convolution_input_parameters, checks_kwargs=convolution_checks_kwargs, overwrite=True )\n",
    "        \n",
    "        # 6.3\n",
    "        # Create a list with all n_pe and compute the average number\n",
    "        dic_pe = {}\n",
    "        list_pe = []\n",
    "        grid_apa = ChannelWsGrid(APA_map[apa], wfset, compute_calib_histo=False)\n",
    "        for run, run_dic in wfset.available_channels.items():\n",
    "            for endpoint, channel_list in run_dic.items():\n",
    "                if endpoint not in dic_pe:\n",
    "                    dic_pe[endpoint] = {}\n",
    "                for channel in channel_list:\n",
    "                    dic_pe[endpoint][channel] = {}\n",
    "                    try:\n",
    "                        for i in range(len(grid_apa.ch_wf_sets[endpoint][channel].waveforms)):\n",
    "                            list_pe.append(grid_apa.ch_wf_sets[endpoint][channel].waveforms[i].analyses[convolution_analysis_label].result['n_pe'])\n",
    "                            dic_pe[endpoint][channel]['n_pe'] = grid_apa.ch_wf_sets[endpoint][channel].waveforms[i].analyses[convolution_analysis_label].result['n_pe']\n",
    "                            dic_pe[endpoint][channel]['e_n_pe'] = grid_apa.ch_wf_sets[endpoint][channel].waveforms[i].analyses[convolution_analysis_label].result['e_n_pe']\n",
    "                            dic_pe[endpoint][channel]['fit_params'] = grid_apa.ch_wf_sets[endpoint][channel].waveforms[i].analyses[convolution_analysis_label].result['fit_params']\n",
    "                            dic_pe[endpoint][channel]['fit_errors'] = grid_apa.ch_wf_sets[endpoint][channel].waveforms[i].analyses[convolution_analysis_label].result['fit_errors']\n",
    "                            dic_pe[endpoint][channel]['r_squared'] = grid_apa.ch_wf_sets[endpoint][channel].waveforms[i].analyses[convolution_analysis_label].result['r_squared']\n",
    "                    except Exception as e:\n",
    "                        print(f'{i_trigger} trigger --> Error in grid_apa.ch_wf_sets[endpoint][channel]: {e}')\n",
    "        \n",
    "        list_pe = [x for x in list_pe if not math.isnan(x)]\n",
    "\n",
    "        for endpoint in list(dic_pe.keys()):\n",
    "            for channel in list(dic_pe[endpoint].keys()):\n",
    "                if math.isnan(dic_pe[endpoint][channel]['n_pe']):\n",
    "                    del dic_pe[endpoint][channel]\n",
    "            if len(dic_pe[endpoint]) == 0:\n",
    "                del dic_pe[endpoint]\n",
    "        \n",
    "        if len(list_pe):\n",
    "            photoelectron_dict[apa]['list'].append(list_pe)\n",
    "            photoelectron_dict[apa]['channel_dic'].append(dic_pe)\n",
    "            photoelectron_dict[apa]['mean'].append(np.mean(list_pe))\n",
    "            photoelectron_dict[apa]['std'].append(np.std(list_pe)) \n",
    "            photoelectron_dict[apa]['n_events'].append(len(list_pe))\n",
    "        else: \n",
    "            photoelectron_dict[apa]['list'].append([])\n",
    "            photoelectron_dict[apa]['channel_dic'].append({})\n",
    "            photoelectron_dict[apa]['mean'].append(np.nan)\n",
    "            photoelectron_dict[apa]['std'].append(np.nan) \n",
    "            photoelectron_dict[apa]['n_events'].append(0)\n",
    "\n",
    "        \n",
    "    photoelectron_dict['trigger time'].append(trigger_time)\n",
    "    print(f\"{i_trigger} trigger --> FS: {photoelectron_dict[1]['n_events'][-1]} events, mean pe = {photoelectron_dict[1]['mean'][-1]:.0f} +/- {photoelectron_dict[1]['std'][-1]:.0f}\\t  ST: {photoelectron_dict[2]['n_events'][-1]:.0f} events, mean pe = {photoelectron_dict[2]['mean'][-1]:.0f} +/- {photoelectron_dict[2]['std'][-1]:.0f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving info in a json (all deconvolution info)\n",
    "with open(f\"{output_analysis_folder}/photoelectron_dic_{energy}GeV.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(prepare_for_json(photoelectron_dict), f, indent=4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving info as Dataframe\n",
    "\n",
    "rows = []\n",
    "for i in range(len(photoelectron_dict['trigger time'])):\n",
    "    row = {\n",
    "        'trigger_time': photoelectron_dict['trigger time'][i],\n",
    "\n",
    "        'apa1_list':       photoelectron_dict[1]['list'][i],\n",
    "        'apa1_mean':       photoelectron_dict[1]['mean'][i],\n",
    "        'apa1_std':        photoelectron_dict[1]['std'][i],\n",
    "        'apa1_n_events':   photoelectron_dict[1]['n_events'][i],\n",
    "\n",
    "        'apa2_list':       photoelectron_dict[2]['list'][i],\n",
    "        'apa2_mean':       photoelectron_dict[2]['mean'][i],\n",
    "        'apa2_std':        photoelectron_dict[2]['std'][i],\n",
    "        'apa2_n_events':   photoelectron_dict[2]['n_events'][i],\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "df_pe = pd.DataFrame(rows)\n",
    "df_pe_name = f\"photoelectron_dataframe_{energy}GeV.csv\"\n",
    "df_pe.to_csv(f\"{output_analysis_folder}/{df_pe_name}\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wnat do read directly the dataset\n",
    "df_pe_name = f\"photoelectron_dataframe_{energy}GeV.csv\"\n",
    "df_pe = pd.read_csv(f\"{output_analysis_folder}/{df_pe_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "apa12_scatter(df_pe, energy, output_analysis_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "apa1_hist_distribution(df_pe, energy, output_analysis_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "apa2_hist_distribution(df_pe, energy, output_analysis_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset histograms - FS\n",
    "timeoffset_distribution_hist(time_info_dic['full_streaming']['offset_list'], 1, energy, output_analysis_folder, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset histograms - ST\n",
    "timeoffset_distribution_hist(time_info_dic['self_trigger']['offset_list'], 2, energy, output_analysis_folder, save = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waffles_NEW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
